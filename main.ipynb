{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fairseq\r\n",
      "  Downloading fairseq-0.10.2-cp36-cp36m-manylinux1_x86_64.whl (1.7 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 1.7 MB 550 kB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: cffi in ./venv/lib/python3.6/site-packages (from fairseq) (1.14.4)\r\n",
      "Collecting sacrebleu>=1.4.12\r\n",
      "  Downloading sacrebleu-1.4.14-py3-none-any.whl (64 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 64 kB 1.3 MB/s eta 0:00:011\r\n",
      "\u001B[?25hCollecting hydra-core\r\n",
      "  Downloading hydra_core-1.0.4-py3-none-any.whl (122 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 122 kB 4.1 MB/s eta 0:00:01     |█████████████▍                  | 51 kB 4.4 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting antlr4-python3-runtime==4.8\r\n",
      "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 112 kB 4.2 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting omegaconf\r\n",
      "  Downloading omegaconf-2.0.5-py3-none-any.whl (36 kB)\r\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.6/site-packages (from omegaconf) (3.7.4.3)\r\n",
      "Collecting PyYAML>=5.1.*\r\n",
      "  Using cached PyYAML-5.3.1.tar.gz (269 kB)\r\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.6/site-packages (from cffi->fairseq) (2.20)\r\n",
      "Collecting cython\r\n",
      "  Downloading Cython-0.29.21-cp36-cp36m-manylinux1_x86_64.whl (2.0 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 2.0 MB 3.9 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting dataclasses\r\n",
      "  Using cached dataclasses-0.8-py3-none-any.whl (19 kB)\r\n",
      "Collecting importlib-resources\r\n",
      "  Downloading importlib_resources-4.1.1-py3-none-any.whl (22 kB)\r\n",
      "Requirement already satisfied: zipp>=0.4 in ./venv/lib/python3.6/site-packages (from importlib-resources->hydra-core) (3.4.0)\r\n",
      "Collecting numpy\r\n",
      "  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 14.8 MB 2.7 MB/s eta 0:00:01    |████████████████                | 7.4 MB 95 kB/s eta 0:01:18     |████████████████▋               | 7.7 MB 95 kB/s eta 0:01:15     |███████████████████████████▏    | 12.6 MB 108 kB/s eta 0:00:21     |███████████████████████████████▌| 14.6 MB 2.7 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting portalocker\r\n",
      "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\r\n",
      "Collecting regex\r\n",
      "  Downloading regex-2020.11.13-cp36-cp36m-manylinux2014_x86_64.whl (723 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 723 kB 3.7 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting torch\r\n",
      "  Using cached torch-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (776.8 MB)\r\n",
      "Collecting tqdm\r\n",
      "  Downloading tqdm-4.55.1-py2.py3-none-any.whl (68 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 68 kB 2.8 MB/s eta 0:00:011\r\n",
      "\u001B[?25hUsing legacy 'setup.py install' for antlr4-python3-runtime, since package 'wheel' is not installed.\r\n",
      "Using legacy 'setup.py install' for PyYAML, since package 'wheel' is not installed.\r\n",
      "Installing collected packages: PyYAML, dataclasses, portalocker, omegaconf, numpy, importlib-resources, antlr4-python3-runtime, tqdm, torch, sacrebleu, regex, hydra-core, cython, fairseq\r\n",
      "    Running setup.py install for PyYAML ... \u001B[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001B[?25h    Running setup.py install for antlr4-python3-runtime ... \u001B[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001B[?25hSuccessfully installed PyYAML-5.3.1 antlr4-python3-runtime-4.8 cython-0.29.21 dataclasses-0.8 fairseq-0.10.2 hydra-core-1.0.4 importlib-resources-4.1.1 numpy-1.19.5 omegaconf-2.0.5 portalocker-2.0.0 regex-2020.11.13 sacrebleu-1.4.14 torch-1.7.1 tqdm-4.55.1\r\n",
      "Cloning into 'fairseq'...\r\n",
      "remote: Enumerating objects: 67, done.\u001B[K\r\n",
      "remote: Counting objects: 100% (67/67), done.\u001B[K\r\n",
      "remote: Compressing objects: 100% (54/54), done.\u001B[K\r\n",
      "Receiving objects:  99% (20453/20659), 8.89 MiB | 1.04 MiB/s   \r  Receiving objects:  16% (3306/20659), 3.09 MiB | 1.18 MiB/s   Receiving objects:  17% (3513/20659), 3.09 MiB | 1.18 MiB/s   Receiving objects:  19% (3926/20659), 3.62 MiB | 1.16 MiB/s   Receiving objects:  21% (4339/20659), 3.62 MiB | 1.16 MiB/s   Receiving objects:  23% (4752/20659), 4.09 MiB | 1.13 MiB/s   Receiving objects:  24% (4959/20659), 4.09 MiB | 1.13 MiB/s   Receiving objects:  26% (5372/20659), 4.59 MiB | 1.11 MiB/s   Receiving objects:  28% (5785/20659), 4.59 MiB | 1.11 MiB/s   Receiving objects:  30% (6198/20659), 4.59 MiB | 1.11 MiB/s   Receiving objects:  32% (6611/20659), 5.11 MiB | 1.10 MiB/s   Receiving objects:  33% (6818/20659), 5.11 MiB | 1.10 MiB/s   Receiving objects:  35% (7231/20659), 5.63 MiB | 1.17 MiB/s   Receiving objects:  37% (7644/20659), 5.63 MiB | 1.17 MiB/s   Receiving objects:  40% (8264/20659), 5.63 MiB | 1.17 MiB/s   Receiving objects:  43% (8884/20659), 6.17 MiB | 1.06 MiB/s   Receiving objects:  46% (9504/20659), 6.17 MiB | 1.06 MiB/s   Receiving objects:  47% (9738/20659), 6.17 MiB | 1.06 MiB/s   Receiving objects:  49% (10123/20659), 6.17 MiB | 1.06 MiB/s   Receiving objects:  51% (10537/20659), 6.71 MiB | 1.06 MiB/s   Receiving objects:  53% (10950/20659), 6.71 MiB | 1.06 MiB/s   Receiving objects:  56% (11570/20659), 6.71 MiB | 1.06 MiB/s   Receiving objects:  59% (12189/20659), 6.71 MiB | 1.06 MiB/s   Receiving objects:  61% (12602/20659), 7.28 MiB | 1.04 MiB/s   Receiving objects:  63% (13016/20659), 7.28 MiB | 1.04 MiB/s   Receiving objects:  65% (13429/20659), 7.28 MiB | 1.04 MiB/s   Receiving objects:  66% (13719/20659), 7.28 MiB | 1.04 MiB/s   Receiving objects:  69% (14255/20659), 7.28 MiB | 1.04 MiB/s   Receiving objects:  71% (14668/20659), 7.79 MiB | 1.03 MiB/s   Receiving objects:  75% (15495/20659), 7.79 MiB | 1.03 MiB/s   Receiving objects:  77% (15908/20659), 7.79 MiB | 1.03 MiB/s   Receiving objects:  79% (16321/20659), 8.37 MiB | 1.03 MiB/s   Receiving objects:  82% (16941/20659), 8.37 MiB | 1.03 MiB/s   Receiving objects:  83% (17327/20659), 8.37 MiB | 1.03 MiB/s   Receiving objects:  87% (17974/20659), 8.37 MiB | 1.03 MiB/s   Receiving objects:  89% (18387/20659), 8.89 MiB | 1.04 MiB/s   Receiving objects:  94% (19420/20659), 8.89 MiB | 1.04 MiB/s   Receiving objects:  97% (20040/20659), 8.89 MiB | 1.04 MiB/s   remote: Total 20659 (delta 24), reused 27 (delta 13), pack-reused 20592\u001B[K\r\n",
      "Receiving objects: 100% (20659/20659), 9.28 MiB | 1.08 MiB/s, done.\r\n",
      "Resolving deltas: 100% (15414/15414), done.\r\n",
      "Wed Jan  6 13:07:34 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 206...  Off  | 00000000:08:00.0 Off |                  N/A |\r\n",
      "| 34%   44C    P8    18W / 175W |    177MiB /  7979MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce RTX 206...  Off  | 00000000:09:00.0 Off |                  N/A |\r\n",
      "| 34%   44C    P2    38W / 175W |    331MiB /  7982MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      4895      C   ...g-Project/venv/bin/python       87MiB |\r\n",
      "|    0   N/A  N/A     17808      C   ...g-Project/venv/bin/python       87MiB |\r\n",
      "|    1   N/A  N/A      4895      C   ...g-Project/venv/bin/python      241MiB |\r\n",
      "|    1   N/A  N/A     17808      C   ...g-Project/venv/bin/python       87MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "%cd '/content/drive/My Drive/Colab data/ROBERTA'\n",
    "!pip install omegaconf hydra-core fairseq\n",
    "!git clone https://github.com/pytorch/fairseq.git\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-01-06 13:08:26--  https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\r\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\r\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 1042301 (1018K) [text/plain]\r\n",
      "Saving to: ‘encoder.json’\r\n",
      "\r\n",
      "encoder.json        100%[===================>]   1018K  1000KB/s    in 1.0s    \r\n",
      "\r\n",
      "2021-01-06 13:08:29 (1000 KB/s) - ‘encoder.json’ saved [1042301/1042301]\r\n",
      "\r\n",
      "--2021-01-06 13:08:29--  https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe\r\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\r\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 456318 (446K) [text/plain]\r\n",
      "Saving to: ‘vocab.bpe’\r\n",
      "\r\n",
      "vocab.bpe           100%[===================>] 445.62K   536KB/s    in 0.8s    \r\n",
      "\r\n",
      "2021-01-06 13:08:31 (536 KB/s) - ‘vocab.bpe’ saved [456318/456318]\r\n",
      "\r\n",
      "--2021-01-06 13:08:31--  https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt\r\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\r\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 603290 (589K) [text/plain]\r\n",
      "Saving to: ‘dict.txt’\r\n",
      "\r\n",
      "dict.txt            100%[===================>] 589.15K   633KB/s    in 0.9s    \r\n",
      "\r\n",
      "2021-01-06 13:08:33 (633 KB/s) - ‘dict.txt’ saved [603290/603290]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget -O encoder.json https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\n",
    "!wget -O vocab.bpe https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe\n",
    "!wget -O dict.txt https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LINES= !cat ROMAN_URDU_DATASET.txt | wc -l\n",
    "LINES = int(LINES[0]) + 1\n",
    "TRAIN=int(498*LINES/500)\n",
    "TEST=int(LINES/500)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('{ head -n '+str(TRAIN)+' > ROMAN_URDU_DATASET.train.txt; head -n  '+str(TEST)+' > ROMAN_URDU_DATASET.valid.txt;   head -n  '+str(TEST)+' > ROMAN_URDU_DATASET.test.txt; } < ROMAN_URDU_DATASET.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%cd 'fairseq'\n",
    "!for SPLIT in train valid test; do \\\n",
    "  python -m examples.roberta.multiprocessing_bpe_encoder \\\n",
    "        --encoder-json ../encoder.json \\\n",
    "        --vocab-bpe ../vocab.bpe \\\n",
    "        --inputs ../ROMAN_URDU_DATASET.${SPLIT}.txt \\\n",
    "        --outputs ../ROMAN_URDU_DATASET.${SPLIT}.bpe \\\n",
    "        --keep-empty \\\n",
    "        --workers 60; \\\n",
    "done\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!fairseq-preprocess \\\n",
    "    --only-source \\\n",
    "    --srcdict ../dict.txt \\\n",
    "    --trainpref ../ROMAN_URDU_DATASET.train.bpe \\\n",
    "    --validpref ../ROMAN_URDU_DATASET.valid.bpe \\\n",
    "    --testpref ../ROMAN_URDU_DATASET.test.bpe \\\n",
    "    --destdir ../data-bin/ \\\n",
    "    --workers 60"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TOTAL_UPDATES=125000    # Total number of training steps\n",
    "WARMUP_UPDATES=10000    # Warmup the learning rate over this many updates\n",
    "PEAK_LR=0.0005          # Peak learning rate, adjust as needed\n",
    "TOKENS_PER_SAMPLE=512   # Max sequence length\n",
    "MAX_POSITIONS=512       # Num. positional embeddings (usually same as above)\n",
    "MAX_SENTENCES=16        # Number of sequences per batch (batch size)\n",
    "UPDATE_FREQ=16          # Increase the batch size 16x\n",
    "\n",
    "DATA_DIR='data-bin'\n",
    "\n",
    "!fairseq-train --fp16 $DATA_DIR \\\n",
    "    --task masked_lm --criterion masked_lm \\\n",
    "    --arch roberta_base --sample-break-mode complete --tokens-per-sample $TOKENS_PER_SAMPLE \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 0.0 \\\n",
    "    --lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\n",
    "    --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n",
    "    --batch-size $MAX_SENTENCES --update-freq $UPDATE_FREQ \\\n",
    "    --max-update $TOTAL_UPDATES --log-format simple --log-interval 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}